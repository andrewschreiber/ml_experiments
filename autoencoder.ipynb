{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_path = os.getcwd() + \"/data/\"\n",
    "frames = np.load(data_path + \"cartpole_frames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(a, window=4, step_size=1):\n",
    "    '''\n",
    "    Input is list of `shape` np arrays of length N\n",
    "    Output is N - 4 x 4 x `shape`\n",
    "    '''\n",
    "    end = a.shape[0]\n",
    "    #return np.moveaxis(np.stack([a[i:end-window+i+1:step_size] for i in range(window)]), 0, -1)\n",
    "    # TimeDistributed looks at axis 1\n",
    "    return np.moveaxis(np.stack([a[i:end-window+i+1:step_size] for i in range(window)]), 0, 1)\n",
    "\n",
    "def eps_to_stacked_window(a, offset=False):\n",
    "    if offset:\n",
    "        return np.vstack([sliding_window(np.stack(x))[1:] for x in a])\n",
    "    else:\n",
    "        return np.vstack([sliding_window(np.stack(x))[:-1] for x in a])\n",
    "\n",
    "windowed_frames = np.expand_dims(eps_to_stacked_window(frames), -1)\n",
    "windowed_frames_next = np.expand_dims(eps_to_stacked_window(frames, offset=True), -1)\n",
    "# windowed_actions = eps_to_stacked_window(actions)\n",
    "assert windowed_frames_next.shape == windowed_frames.shape\n",
    "print(windowed_frames.shape)\n",
    "\n",
    "stk_frames = torch.from_numpy(windowed_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda ready\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 549.210693\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 136.647308\n",
      "====> Epoch: 1 Average loss: 148.1090\n",
      "====> Test set loss: 119.4382\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 122.028915\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 114.360924\n",
      "====> Epoch: 2 Average loss: 116.0095\n",
      "====> Test set loss: 111.9825\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 118.039490\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 111.539803\n",
      "====> Epoch: 3 Average loss: 111.5371\n",
      "====> Test set loss: 109.7136\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 114.115250\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 112.590012\n",
      "====> Epoch: 4 Average loss: 109.4967\n",
      "====> Test set loss: 107.9548\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 104.463951\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 109.390930\n",
      "====> Epoch: 5 Average loss: 108.2922\n",
      "====> Test set loss: 107.7711\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 110.807755\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 112.367950\n",
      "====> Epoch: 6 Average loss: 107.4781\n",
      "====> Test set loss: 106.6117\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 105.240112\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 109.459625\n",
      "====> Epoch: 7 Average loss: 106.8355\n",
      "====> Test set loss: 106.0358\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 106.275894\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 105.342743\n",
      "====> Epoch: 8 Average loss: 106.3615\n",
      "====> Test set loss: 105.7117\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 101.945076\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 109.268929\n",
      "====> Epoch: 9 Average loss: 105.9501\n",
      "====> Test set loss: 105.3282\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 100.688957\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 101.428635\n",
      "====> Epoch: 10 Average loss: 105.6491\n",
      "====> Test set loss: 105.3477\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 111.462250\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 106.024887\n",
      "====> Epoch: 11 Average loss: 105.3947\n",
      "====> Test set loss: 104.9564\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 102.637634\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 104.830826\n",
      "====> Epoch: 12 Average loss: 105.0921\n",
      "====> Test set loss: 104.5409\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 101.178162\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 104.292892\n",
      "====> Epoch: 13 Average loss: 104.8660\n",
      "====> Test set loss: 104.6204\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 105.503487\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 104.549484\n",
      "====> Epoch: 14 Average loss: 104.7004\n",
      "====> Test set loss: 104.5254\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 104.455574\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 105.266556\n",
      "====> Epoch: 15 Average loss: 104.4896\n",
      "====> Test set loss: 104.4479\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 106.151871\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 104.503563\n",
      "====> Epoch: 16 Average loss: 104.3632\n",
      "====> Test set loss: 103.9246\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 104.458260\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 101.068100\n",
      "====> Epoch: 17 Average loss: 104.1748\n",
      "====> Test set loss: 104.2414\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 102.083145\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 106.920975\n",
      "====> Epoch: 18 Average loss: 103.9977\n",
      "====> Test set loss: 103.6832\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 107.539238\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 105.645531\n",
      "====> Epoch: 19 Average loss: 103.8521\n",
      "====> Test set loss: 103.7151\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 102.973946\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 105.638145\n",
      "====> Epoch: 20 Average loss: 103.7122\n",
      "====> Test set loss: 103.7226\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 103.110741\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 100.315590\n",
      "====> Epoch: 21 Average loss: 103.6046\n",
      "====> Test set loss: 103.7094\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 97.248688\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 100.860382\n",
      "====> Epoch: 22 Average loss: 103.4992\n",
      "====> Test set loss: 103.5537\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 98.518242\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 100.769112\n",
      "====> Epoch: 23 Average loss: 103.3780\n",
      "====> Test set loss: 103.3775\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 96.626801\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 105.447906\n",
      "====> Epoch: 24 Average loss: 103.2850\n",
      "====> Test set loss: 103.0235\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 106.707573\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 96.157860\n",
      "====> Epoch: 25 Average loss: 103.1696\n",
      "====> Test set loss: 103.4569\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 102.756882\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 95.783463\n",
      "====> Epoch: 26 Average loss: 103.0707\n",
      "====> Test set loss: 103.2731\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 100.851440\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 101.855392\n",
      "====> Epoch: 27 Average loss: 102.9976\n",
      "====> Test set loss: 103.2799\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 107.127197\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 105.626503\n",
      "====> Epoch: 28 Average loss: 102.8708\n",
      "====> Test set loss: 103.1913\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 100.552116\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 98.351730\n",
      "====> Epoch: 29 Average loss: 102.8531\n",
      "====> Test set loss: 102.9817\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 101.754028\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 102.620575\n",
      "====> Epoch: 30 Average loss: 102.7853\n",
      "====> Test set loss: 102.7567\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 99.591522\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 99.788872\n",
      "====> Epoch: 31 Average loss: 102.6568\n",
      "====> Test set loss: 102.6870\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 102.946831\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 100.056564\n",
      "====> Epoch: 32 Average loss: 102.5868\n",
      "====> Test set loss: 102.6421\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 105.048843\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 99.037933\n",
      "====> Epoch: 33 Average loss: 102.5298\n",
      "====> Test set loss: 102.6773\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 96.200401\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 101.363876\n",
      "====> Epoch: 34 Average loss: 102.4559\n",
      "====> Test set loss: 102.5617\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 99.252419\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 101.544334\n",
      "====> Epoch: 35 Average loss: 102.4107\n",
      "====> Test set loss: 102.4687\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 105.843491\n",
      "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 105.576988\n",
      "====> Epoch: 36 Average loss: 102.3119\n",
      "====> Test set loss: 102.3924\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 102.323776\n",
      "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 103.222565\n",
      "====> Epoch: 37 Average loss: 102.2977\n",
      "====> Test set loss: 102.4606\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 98.238480\n",
      "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 100.431183\n",
      "====> Epoch: 38 Average loss: 102.2522\n",
      "====> Test set loss: 102.2283\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 100.208405\n",
      "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 101.543571\n",
      "====> Epoch: 39 Average loss: 102.1802\n",
      "====> Test set loss: 102.3397\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 104.153442\n",
      "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 96.998581\n",
      "====> Epoch: 40 Average loss: 102.1464\n",
      "====> Test set loss: 102.1611\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 97.882599\n",
      "Train Epoch: 41 [32000/60000 (53%)]\tLoss: 94.839890\n",
      "====> Epoch: 41 Average loss: 102.0762\n",
      "====> Test set loss: 102.2339\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 99.819969\n",
      "Train Epoch: 42 [32000/60000 (53%)]\tLoss: 101.015472\n",
      "====> Epoch: 42 Average loss: 102.0574\n",
      "====> Test set loss: 102.3479\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 103.688385\n",
      "Train Epoch: 43 [32000/60000 (53%)]\tLoss: 107.200966\n",
      "====> Epoch: 43 Average loss: 101.9813\n",
      "====> Test set loss: 102.1638\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 104.104019\n",
      "Train Epoch: 44 [32000/60000 (53%)]\tLoss: 99.666214\n",
      "====> Epoch: 44 Average loss: 101.9511\n",
      "====> Test set loss: 102.1528\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 96.956863\n",
      "Train Epoch: 45 [32000/60000 (53%)]\tLoss: 101.267731\n",
      "====> Epoch: 45 Average loss: 101.9195\n",
      "====> Test set loss: 102.2165\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 101.905708\n",
      "Train Epoch: 46 [32000/60000 (53%)]\tLoss: 104.881714\n",
      "====> Epoch: 46 Average loss: 101.8758\n",
      "====> Test set loss: 102.0263\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 107.955269\n",
      "Train Epoch: 47 [32000/60000 (53%)]\tLoss: 104.007179\n",
      "====> Epoch: 47 Average loss: 101.8357\n",
      "====> Test set loss: 102.1550\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 102.548141\n",
      "Train Epoch: 48 [32000/60000 (53%)]\tLoss: 104.404495\n",
      "====> Epoch: 48 Average loss: 101.7422\n",
      "====> Test set loss: 102.1294\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 98.371613\n",
      "Train Epoch: 49 [32000/60000 (53%)]\tLoss: 105.673660\n",
      "====> Epoch: 49 Average loss: 101.6848\n",
      "====> Test set loss: 101.9793\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 98.186287\n",
      "Train Epoch: 50 [32000/60000 (53%)]\tLoss: 102.837143\n",
      "====> Epoch: 50 Average loss: 101.6642\n",
      "====> Test set loss: 102.1853\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda ready\")\n",
    "    \n",
    "USE_CUDA = True\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if USE_CUDA else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "train_loader =\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if USE_CUDA:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # see Appendix B:\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "LOG_INTERVAL = 500\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if USE_CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in test_loader:\n",
    "        if USE_CUDA:\n",
    "            data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5bcb31f510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, classes = data\n",
    "    first_img = inputs[0].numpy()\n",
    "    inputs, classes = Variable(inputs.resize_(BATCH_SIZE, 784)).cuda(), Variable(classes).cuda()\n",
    "    a=fig.add_subplot(1,2,1)\n",
    "    plt.imshow(model(inputs)[0].data[0].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "    a=fig.add_subplot(1,2,2)\n",
    "    plt.imshow(first_img.reshape(28,28), cmap='gray')\n",
    "    print(classes[0])\n",
    "    plt.show(block=True)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
